Inter-Annotator Agreement Statistics
====================================
Total Samples: 2000
Annotators: 3 (Experts A, B, C)

Metrics:
1. Fleiss' Kappa: 0.82 (Strong Agreement)
2. Pairwise Agreement (Avg): 89.5%
3. Major Disagreements: 4.2% (Resolved via discussion)

Category Breakdown:
- Relation Extraction Agreement: 0.85
- Entity Boundary Agreement: 0.91
- Entity Type Agreement: 0.88